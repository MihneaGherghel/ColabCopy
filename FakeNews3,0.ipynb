{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX+rnoGy21kO4X3VUsq3w/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihneaGherghel/ColabCopy/blob/main/FakeNews3%2C0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I4j4ePeYpJS",
        "outputId": "f681b821-6ceb-491e-9d37-feb886d595a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OptainData:\n",
        "  def __init__(self,file_path):\n",
        "    self.data = pd.read_csv(file_path)\n",
        "    self.data['label'] = pd.to_numeric(self.data['label'], errors='coerce')\n",
        "    self.data = self.data.dropna()\n",
        "    self.data['label'] = self.data['label'].astype(int)\n",
        "    self.data.reset_index(inplace=True)\n",
        "\n",
        "  def split_labels_samples(self):\n",
        "    samples=self.data.drop('label',axis=1)\n",
        "    labels=self.data['label']\n",
        "    return samples,labels,self.data"
      ],
      "metadata": {
        "id": "ZMnUPEwWcC2I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=OptainData('./gdrive/MyDrive/train.csv/FakeNews.csv')\n",
        "samples,labels,samples_labels=data.split_labels_samples()"
      ],
      "metadata": {
        "id": "9BGT0Jl8cGt6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh0sSCx5cINp",
        "outputId": "7509601d-2436-4788-8c8e-e29f8b424329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages=samples_labels.copy()\n",
        "messages.reset_index(inplace=True)\n",
        "corpus=[]\n",
        "ps=PorterStemmer()\n",
        "for i in range(0,len(messages)):\n",
        "  review=re.sub('[^a-zA-Z]',' ', messages['title'][i])\n",
        "  review=review.lower()\n",
        "  review=review.split()\n",
        "  review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "  review=' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "metadata": {
        "id": "YrZTbMkycMJL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "IFSmfnhUyTx1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_size=5000\n",
        "onehot_repr=[one_hot(words,voc_size) for words in corpus]"
      ],
      "metadata": {
        "id": "Ay6W_ecxya_e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_length=20\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)"
      ],
      "metadata": {
        "id": "ABvZmENoydik"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "oApty2Nfylb3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(messages['label'])"
      ],
      "metadata": {
        "id": "13lFLLdsmb01"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test= train_test_split(X_final, y_final, test_size=0.33,random_state=42)"
      ],
      "metadata": {
        "id": "av8NCSEgewZs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_tensor = torch.from_numpy(X_train)\n",
        "train_target_tensor = torch.from_numpy(y_train)\n",
        "test_input_tensor = torch.from_numpy(X_test)\n",
        "test_target_tensor = torch.from_numpy(y_test)"
      ],
      "metadata": {
        "id": "bBRq7iZMlpcU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(train_input_tensor, train_target_tensor)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_input_tensor, test_target_tensor)"
      ],
      "metadata": {
        "id": "CpnJMqVZm1xa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "iiIokFa7nBVZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "23-05b8FezQy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMFakeNewsClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMFakeNewsClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.linear = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embeds = self.embedding(text)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output = self.linear(lstm_out[:, -1])\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "e7iK0RM0e8RN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LSTMFakeNewsClassifier(vocab_size=voc_size, embedding_dim=100, hidden_dim=258)"
      ],
      "metadata": {
        "id": "ZXIr5__-e9uS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "YZHxuyKnguUv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numarcazuricorecte=0\n",
        "for epoch in range(5):\n",
        "    for text,label in train_loader:\n",
        "        # Zero the gradients\n",
        "        label=label.float()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model(text).float()\n",
        "        loss = criterion(outputs, label)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print progress\n",
        "    correct=0\n",
        "    for text,label in test_loader:\n",
        "      outputs = model(text)\n",
        "      predictions = outputs > 0.5\n",
        "      labels = label > 0.5\n",
        "      c = [labels[i]==predictions[i][0] for i in range(0,len(outputs),1)]\n",
        "      for i in c:\n",
        "        if i==True:\n",
        "          correct+=1\n",
        "    print( print('Epoch: [{}/{}], Correct: [{}/{}]'.format(\n",
        "                epoch + 1, 5, len(X_test), correct)))\n"
      ],
      "metadata": {
        "id": "yKHXFh-AhNFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}